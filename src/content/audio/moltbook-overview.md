---
title: "When AI Systems Talk to Each Other, Safety Breaks Down"
description: "Multi-agent AI research reveals a critical gap: single-agent safety does not compose. 1.5M interactions show 46.34% attack success rates."
date: 2026-02-13
tags: ["notebooklm", "ai", "safety", "research", "multi-agent"]
audioUrl: "/notebook-assets/failure-first/moltbook/audio.mp3"
duration: "14:32"
relatedPost: "when-ai-systems-talk-safety-breaks"
---

NotebookLM Studio conversation exploring Moltbook research — a simulated social ecosystem revealing how multi-agent systems break in ways single agents never do. Two AI hosts discuss the central finding: single-agent safety does not compose.

The research analysed over 1.5 million interactions in a collaborative environment where agents share context, plugins, and trust. Key findings: 46.34% baseline attack success rate for prompt injection (significantly higher than single-agent baselines), and a median 16-minute window to critical security failure when no specialized defenses are present.

The core insight: in multi-agent systems, content becomes code. When an agent reads a post or comment, it doesn't just display it — it reasons over it, potentially executing actions based on it. This transforms shared content into semantic worms that exploit reasoning logic rather than buffer overflows.

The takeaway: you can't build safe multi-agent systems by stitching together individually safe agents.

**Generated by**: NotebookLM Studio
**Source materials**: Moltbook dataset analysis, multi-agent security framework, semantic worm taxonomy

[Read the full research article →](/blog/when-ai-systems-talk-safety-breaks/)
